{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Keras y TensorFlow\n",
    "\n",
    "TensorFlow (TF) es una plataforma basada en Python, gratuita y de código abierto, desarrollada principalmente por Google. Similar a NumPy, el propósito principal de TensorFlow es permitir al usuario manipular expresiones matemáticas en tensores numéricos.\n",
    "\n",
    "Keras es una API (Application Programming Interface) para Python, construida sobre TensorFlow, que provee una manera conveniente para definir y entrenar cualquier tipo de modelo de DL. \n",
    "\n",
    "## Tensores constantes y variables\n",
    "\n",
    "Para hacer algo en TF, necesitamos algunos tensores. Los tensores deben crearse con algún valor inicial. Por ejemplo, podríamos crear tensores que sólo contengan unos o ceros, o tensores que tengan valores de alguna distribución aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.ones(shape = (2,1))  # Equivalente a np.ones(shape=(2,1))\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.zeros(shape = (3,3)) # Equivalente a np.zeros(shape=(2,1))\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un tensor con valores aleatorios a partir de una distribución\n",
    "# normal con media = 0 y desviación estándar = 1\n",
    "\n",
    "c = tf.random.normal(shape = (3,1), mean = 0.0, stddev = 1.0)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.random.uniform(shape = (3,1), minval = 0., maxval = 1.)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar un modelo, necesitamos actualizar su estado, que es un conjunto de tensores. Como los tensores no son asignables, tenemos que utilizar *variables* con la clase `tf.Variable`, que nos permite modificar el estado en TF. \n",
    "\n",
    "Para crear una variable, necesitamos dar un valor inicial, como un tensor aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable(initial_value = tf.random.normal(shape = (3,1)))\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estado de la variable se puede modificar a través del método `assign`, como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.assign(tf.ones((3,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También funciona para un subconjunto de entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[0,0].assign(3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones tensoriales\n",
    "\n",
    "TF ofrece un conjunto de operaciones tensoriales para expresar fórmulas matemáticas. Por ejemplo, si queremos obtener el cuadrado de las entradas de un tensor, usamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sq = tf.square(d)\n",
    "\n",
    "d_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para sacar la raíz cuadrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sqrt = tf.sqrt(d)\n",
    "\n",
    "d_sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumamos dos tensores, entrada a entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma = c + d\n",
    "\n",
    "suma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Tape\n",
    "\n",
    "`GradientTape` es una API que nos permite utilizar las capacidades de diferenciación automática de TF. Es un esquema de Python que va a \"registrar\" las operaciones tensoriales que corran dentro de él, en forma de un grafo computacional (a veces llamado *tape*). Ese grafo puede usarse para recuperar el gradiente de cualquier salida con respecto a cualquier variable o conjunto de variables (las instancias de la clase `tf.Variable`). \n",
    "\n",
    "Una `tf.Variable` es un tipo específico de tensor que debe mantener un estado mutable, por ejemplo, los pesos de una red neuronal siempre son instancias del tipo `tf.Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(0.) # Variable escalar con valor inicial de 0\n",
    "\n",
    "with tf.GradientTape() as tape: # Iniciamos el esquema de GradientTape\n",
    "    y = 2 * x + 3               # Dentro del esquema, aplicamos operaciones tensoriales a la variable\n",
    "\n",
    "dy_dx = tape.gradient(y, x) # Utilizamos la tape para recuperar el gradiente de la salida y respecto a x\n",
    "\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos otro ejemplo, ahora con un tensor un poco más complejo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(tf.random.uniform((2,2))) # Variable de forma (2,2) con valores iniciales todos ceros\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = 2 * x + 3\n",
    "\n",
    "grad_of_y_wrt_x = tape.gradient(y, x)\n",
    "\n",
    "grad_of_y_wrt_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: Clasificador lineal en TensorFlow\n",
    "\n",
    "Ya vimos tensores, variables y operaciones tensoriales, y ya sabemos cómo calcular gradientes. Podemos construir cualquier modelo de ML que se base en el método del gradiente descendente. Veamos.\n",
    "\n",
    "Supongamos que nos piden implementar un clasificador lineal usando TF. Vamos a ir paso a paso, así que vamos a generar datos sintéticos bonitos y separables para trabajar: dos clases de puntos en un plano.\n",
    "\n",
    "Generaremos cada clase de puntos dibujando sus coordenadas a partir de una distribución aleatoria con matriz de covarianza y media específicas. Recordemos que, de forma intuitiva, la matriz de covarianza describe la forma de la nube de puntos, y que la media describe su posición en el plano. \n",
    "\n",
    "Para este ejemplo vamos a utilizar la misma matriz de covarianza para cada una de las nubes de puntos, pero utilizaremos dos medias distintas, por lo que las nubes van a tener la misma forma, pero posiciones distintas.\n",
    "\n",
    "Primero definiremos el número de puntos por clase que queremos, que van a ser, 1000 en cada nube.\n",
    "\n",
    "Después, generamos las muestras \"negativas\" de 1000 puntos aleatorios. La matriz de covarianza coresponde a una nubecita en forma de óvalo orientada desde abajo a la izquierda hacia arriba y a la derecha.\n",
    "\n",
    "El otro conjunto de datos es lo mismo, pero con media distinta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_class = 1000\n",
    "\n",
    "\n",
    "negative_samples = np.random.multivariate_normal(\n",
    "                                                mean = [0, 3],\n",
    "                                                cov = [[1, 0.5], [0.5, 1]],\n",
    "                                                size = num_samples_per_class)\n",
    "\n",
    "positive_samples = np.random.multivariate_normal(\n",
    "                                                mean = [3, 0],\n",
    "                                                cov = [[1, 0.5], [0.5,1]],\n",
    "                                                size = num_samples_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior, las variables `negative_samples` y `positive_samples` son arrays de forma (1000,2). Pongamos uno sobre otro para formar un sólo array de (2000,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, tenemos que generar también las correspondientes etiquetas objetivo, que en este caso serán un array de ceros y unos de forma (2000,1), donde `targets[i,0]` va a ser 0 si `inputs[i]` pertenece a la clase 0 (y visceversa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype = 'float32'),\n",
    "                    np.ones((num_samples_per_class, 1), dtype = 'float32')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiquemos los datos con Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(inputs[:,0], inputs[:,1], c = targets[:,0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien, tenemos los datos de entrada listos. Ahora crearemos un clasificador lineal que aprenda a separar estos dos aglomerados. Un clasificador lineal es una transformación afín (`predicción = W • entrada + b`) entrenado para minimizar el cuadrado de la diferencia entre las predicciones y los objetivos.\n",
    "\n",
    "Creamos entonces nuestras variables `w` y `b`, inicializando con valores aleatorios y con ceros, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2 # Las entradas son puntos en 2 dimensiones\n",
    "\n",
    "output_dim = 1 # Mientras que la salida será 0 o 1\n",
    "\n",
    "w = tf.Variable(initial_value = tf.random.uniform(shape = (input_dim, output_dim)))\n",
    "\n",
    "b = tf.Variable(initial_value = tf.zeros(shape = (output_dim)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de paso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(inputs):\n",
    "    return tf.matmul(inputs, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como nuestro clasificador lineal opera en entradas de 2 dimensiones, `w` en realidad está formado por dos coeficientes escalares, `w1` y `w2`: `w = [[w1], [w2]]`.\n",
    "\n",
    "Mientras que `b` es un solo coeficiente escalar. Por lo que, para cada dato de entrada `[x,y]`, su predicción será: `prediction = [[w1]. [w2]] • [x,y] + b = w1 * x + w2 * y + b`.\n",
    "\n",
    "Ahora, definimos la función de costo, que en este caso corresponde al error cuadrático medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(targets, predictions):\n",
    "    per_sample_losses = tf.square(targets - predictions)\n",
    "    return tf.reduce_mean(per_sample_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`per_sample_losses` será un tensor con la misma forma que los objetivos y las predicciones y que va a contener los scores de la función de costo.\n",
    "\n",
    "El método `tf.reduce_mean()` promedia todos los valores de la función de costo y nos da un sólo número.\n",
    "\n",
    "Ahora sigue el paso de entrenamiento, donde se va a recibir un poco de datos de entrenamiento y se van a actualizar los pesos `w` y el valor `b` para reducir el costo sobre los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "def training_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = square_loss(predictions, targets)\n",
    "    grad_loss_wrt_w, grad_loss_wrt_b = tape.gradient(loss, [w,b])\n",
    "    w.assign_sub(grad_loss_wrt_w * learning_rate)\n",
    "    b.assign_sub(grad_loss_wrt_b * learning_rate)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por simplicidad, se hará *batch training* en vez de *mini-batch training*: vamos a correr cada paso de entrenamiento (calcular el gradiente y actualizar los pesos) para todos los datos, en vez de iterar en pequeños conjuntos del conjunto completo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(40):\n",
    "    loss = training_step(inputs, targets)\n",
    "    print(f'Loss at step {step}: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de 40 iteraciones, el costo del entrenamiento parece estabilizarse alrededor de 0.0255. Grafiquemos cómo nuestro modelo lineal clasifica los datos de entrenamiento.\n",
    "\n",
    "Como nuestros objetivos son ceros y unos, una entrada dada puede ser clasificada como \"0\", si su valor de predicción es menor a 0.5, y como \"1\" si está por arriba de 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(inputs)\n",
    "\n",
    "plt.scatter(inputs[:,0], inputs[:,1], c = predictions[:,0] > 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casi no se nota la diferencia entre las predicciones del modelo (gráfica de arriba) y los datos de entrada. \n",
    "\n",
    "Recordemos que el valor de la predicción para cualquier punto $(x,y)$ es simplemente:\n",
    "\n",
    "$$ \\hat{y} = w_1 x + w_2 y + b  $$\n",
    "\n",
    "Y las clases a las que se clasifica la predicción son tales que:\n",
    "\n",
    "$$ \\hat{y} = \\begin{cases}\n",
    "            0 & \\text{si} & w_1 x + w_2 y + b < 0.5 \\\\\n",
    "            1 & \\text{si} & w_1 x + w_2 y + b > 0.5 \\\\\n",
    "\n",
    "\\end{cases} $$\n",
    "\n",
    "Pero pues esto equivale a la ecuación de una recta:\n",
    "\n",
    "$$  w_1 x + w_2 y + b = 0.5 \\quad \\to \\quad y = - \\frac{w_1}{w_2} x + \\frac{0.5 - b}{w_2}$$\n",
    "\n",
    "Arriba de la línea está la clase 1 y por debajo la clase 0. \n",
    "\n",
    "Grafiquemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 4, 100)\n",
    "\n",
    "y = -w[0] / w[1] * x + (0.5 - b) / w[1]\n",
    "\n",
    "plt.plot(x,y, \"-r\")\n",
    "plt.scatter(inputs[:,0], inputs[:,1], c = predictions[:,0] > 0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esto se trata la tarea de un clasificador. Encuentra los parámetros de una línea (o, en muchas dimensiones, un hiperplano) que logre separar adecuadamente dos tipos de datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
